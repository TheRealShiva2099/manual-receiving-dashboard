"""Local dashboard server for Manual Receiving ATC.

Design goals:
- localhost only (no network exposure)
- dead simple (YAGNI)
- serve an HTML file generated by the main app
- provide /api/events for future front-end improvements

Keep it small. Keep it readable. Zen puppy approved.
"""

from __future__ import annotations

import json
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import Any

from flask import Flask, Response, jsonify, request, send_file


def _get_env_int(name: str, default: int) -> int:
    try:
        return int(os.environ.get(name, str(default)))
    except ValueError:
        return default


def _load_config(base_dir: Path) -> dict[str, Any]:
    cfg_path = base_dir / "atc_config.json"
    return json.loads(cfg_path.read_text(encoding="utf-8"))


def _bq_cmd(cfg: dict[str, Any]) -> list[str]:
    bq_path = str(cfg.get("bigquery", {}).get("bq_path", "bq")).strip() or "bq"
    job_project = cfg.get("bigquery", {}).get("job_project")
    billing_project = cfg.get("bigquery", {}).get("billing_project")
    project_id = job_project or billing_project

    base_args = [
        "query",
        "--quiet",
        "--use_legacy_sql=false",
        "--format=json",
    ]
    if project_id:
        base_args.append(f"--project_id={project_id}")

    return _resolve_bq_argv(bq_path) + base_args


def _resolve_bq_argv(bq_path: str) -> list[str]:
    """Return argv to invoke bq reliably.

    If bq_path is a Cloud SDK .cmd launcher, call the underlying
    `bin/bootstrapping/bq.py` directly.
    """

    p = Path(bq_path)
    if p.suffix.lower() in {".cmd", ".bat"}:
        cloudsdk_root = p.parent.parent
        bq_py = cloudsdk_root / "bin" / "bootstrapping" / "bq.py"
        bundled_python = cloudsdk_root / "platform" / "bundledpython" / "python.exe"
        python_exe = str(bundled_python) if bundled_python.exists() else sys.executable

        if not bq_py.exists():
            raise FileNotFoundError(f"Cloud SDK bq.py not found: {bq_py}")

        return [python_exe, str(bq_py)]

    return [bq_path]


def _run_bq_json(cfg: dict[str, Any], sql: str, timeout_s: int = 600) -> list[dict[str, Any]]:
    cmd = _bq_cmd(cfg)
    proc = subprocess.run(
        cmd,
        input=sql,
        capture_output=True,
        text=True,
        timeout=timeout_s,
        check=False,
        shell=False,
    )
    if proc.returncode != 0:
        details = (proc.stderr or proc.stdout or "").strip()
        raise RuntimeError(details or f"bq failed (exit={proc.returncode})")

    out = proc.stdout.strip()
    if not out:
        return []

    payload = json.loads(out)
    if not isinstance(payload, list):
        raise RuntimeError("Unexpected bq json output")
    return payload


def create_app(base_dir: Path) -> Flask:
    app = Flask(__name__)

    dashboard_path = base_dir / "atc_dashboard.html"
    template_path = base_dir / "dashboard_template.html"
    events_log_path = base_dir / "atc_events_log.json"
    status_path = base_dir / "atc_status.json"

    analytics_path = base_dir / "atc_analytics.html"
    analytics_template_path = base_dir / "analytics_template.html"
    viz_path = base_dir / "atc_viz.html"
    viz_template_path = base_dir / "viz_template.html"
    deliveries_path = base_dir / "atc_deliveries.html"
    deliveries_template_path = base_dir / "deliveries_template.html"

    triage_path = base_dir / "atc_delivery_triage.json"

    def _default_triage() -> dict[str, Any]:
        return {"version": 1, "updated_at": None, "deliveries": {}}

    def _load_triage() -> dict[str, Any]:
        if not triage_path.exists():
            return _default_triage()
        try:
            payload = json.loads(triage_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            return _default_triage()
        if not isinstance(payload, dict):
            return _default_triage()
        payload.setdefault("version", 1)
        payload.setdefault("updated_at", None)
        if not isinstance(payload.get("deliveries"), dict):
            payload["deliveries"] = {}
        return payload

    def _save_triage(payload: dict[str, Any]) -> None:
        # Atomic-ish write to avoid corrupting JSON on crash.
        tmp = triage_path.with_suffix(".tmp")
        tmp.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        tmp.replace(triage_path)

    def _normalize_primary_cause(value: Any) -> str:
        s = str(value or "").strip()
        # Back-compat: old label
        if s.lower() == "process deviation":
            return "Not on process"
        return s

    def _upsert_delivery_triage(*, delivery_number: str, updates: dict[str, Any]) -> dict[str, Any]:
        payload = _load_triage()
        deliveries = payload.get("deliveries", {})
        if not isinstance(deliveries, dict):
            deliveries = {}
            payload["deliveries"] = deliveries

        key = str(delivery_number).strip()
        if not key:
            raise ValueError("delivery_number is required")

        current = deliveries.get(key, {})
        if not isinstance(current, dict):
            current = {}

        # Only allow known fields (basic guardrail)
        allowed = {
            "checked",
            "primary_cause",
            "escalation",
            "note",
            "updated_by",
            # QA placeholders (read-only for now, but stored if sent)
            "qa_status",
            "qa_note",
        }
        clean: dict[str, Any] = {k: v for k, v in updates.items() if k in allowed}
        if "primary_cause" in clean:
            clean["primary_cause"] = _normalize_primary_cause(clean.get("primary_cause"))

        current.update(clean)
        current["updated_at_epoch"] = int(time.time())
        deliveries[key] = current
        payload["updated_at"] = time.strftime("%Y-%m-%d %H:%M:%S")

        _save_triage(payload)
        return current

    # Legacy roster (removed): leaving filenames alone so old files can exist without being served.

    @app.get("/")
    def deliveries_home() -> Response:
        # Home page: deliveries history (ops doesn't want to scroll Teams).
        if deliveries_path.exists():
            return send_file(deliveries_path)
        if deliveries_template_path.exists():
            return send_file(deliveries_template_path)
        return Response(
            "Deliveries template missing. Reinstall the ATC package.",
            status=500,
            mimetype="text/plain",
        )

    @app.get("/raw")
    def raw_data() -> Response:
        # Former home page: raw events table.
        if dashboard_path.exists():
            return send_file(dashboard_path)
        if template_path.exists():
            return send_file(template_path)
        return Response(
            "Dashboard template missing. Reinstall the ATC package.",
            status=500,
            mimetype="text/plain",
        )

    @app.get("/analytics")
    def analytics() -> Response:
        if analytics_path.exists():
            return send_file(analytics_path)
        if analytics_template_path.exists():
            return send_file(analytics_template_path)
        return Response(
            "Analytics template missing. Reinstall the ATC package.",
            status=500,
            mimetype="text/plain",
        )

    @app.get("/viz")
    def viz() -> Response:
        if viz_path.exists():
            return send_file(viz_path)
        if viz_template_path.exists():
            return send_file(viz_template_path)
        return Response(
            "Viz template missing. Reinstall the ATC package.",
            status=500,
            mimetype="text/plain",
        )

    @app.get("/deliveries")
    def deliveries_alias() -> Response:
        return deliveries_home()

    @app.get("/api/deliveries")
    def api_deliveries() -> Response:
        """Return delivery-level rollups from the local event log.

        This is the "history table" for ops so they don't have to scroll Teams.
        """

        mode = str(request.args.get("mode", "notified")).strip().lower()
        limit = int(request.args.get("limit", "50") or 50)
        limit = max(1, min(250, limit))

        if not events_log_path.exists():
            return jsonify({"ok": True, "mode": mode, "rows": []})

        try:
            log_payload: dict[str, Any] = json.loads(events_log_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            return jsonify({"ok": True, "mode": mode, "rows": []})

        try:
            email_state: dict[str, Any] = json.loads((base_dir / "atc_email_state.json").read_text(encoding="utf-8"))
        except Exception:
            email_state = {}

        notified_map = email_state.get("emailed_deliveries", {}) if isinstance(email_state, dict) else {}
        if not isinstance(notified_map, dict):
            notified_map = {}

        shift_map = email_state.get("notified_shift_by_delivery", {}) if isinstance(email_state, dict) else {}
        if not isinstance(shift_map, dict):
            shift_map = {}

        triage_payload = _load_triage()
        triage_map = triage_payload.get("deliveries", {}) if isinstance(triage_payload, dict) else {}
        if not isinstance(triage_map, dict):
            triage_map = {}

        events = log_payload.get("events", []) if isinstance(log_payload, dict) else []
        if not isinstance(events, list):
            events = []

        # Optional overflow filter (mirrors /api/events behavior)
        try:
            cfg = _load_config(base_dir)
            overflow = {str(x).strip().upper() for x in (cfg.get("monitoring", {}).get("overflow_locations", []) or [])}
            min_cases_per_delivery = float((cfg.get("deliveries_page", {}) or {}).get("min_cases_per_delivery", 10) or 0)
        except Exception:
            overflow = set()
            min_cases_per_delivery = 0.0

        def norm(s: Any) -> str:
            return str(s or "").strip()

        def safe_float(x: Any) -> float:
            try:
                return float(x)
            except Exception:
                return 0.0

        def to_local(ts_epoch: Any) -> str:
            try:
                return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(int(ts_epoch)))
            except Exception:
                return ""

        # Group by delivery
        by_delivery: dict[str, list[dict[str, Any]]] = {}
        for e in events:
            if not isinstance(e, dict):
                continue
            loc = norm(e.get("location_id"))
            if overflow and loc and loc.upper() in overflow:
                continue
            d = norm(e.get("delivery_number"))
            if not d:
                continue
            by_delivery.setdefault(d, []).append(e)

        rows: list[dict[str, Any]] = []
        for d, evs in by_delivery.items():
            notified_epoch = notified_map.get(d)
            is_notified = isinstance(notified_epoch, (int, float, str)) and str(notified_epoch).strip() != ""

            if mode == "notified" and not is_notified:
                continue

            # Delivery-level aggregates
            shift_counts: dict[str, int] = {}
            locs: set[str] = set()
            total_cases = 0.0

            items: dict[str, dict[str, Any]] = {}
            for e in evs:
                sh = norm(e.get("shift_label")) or "Off Shift"
                shift_counts[sh] = shift_counts.get(sh, 0) + 1

                loc = norm(e.get("location_id"))
                if loc:
                    locs.add(loc)

                c = safe_float(e.get("case_qty"))
                total_cases += c

                item = norm(e.get("item_nbr"))
                if not item:
                    continue
                meta = items.setdefault(
                    item,
                    {
                        "item_nbr": item,
                        "item_desc": norm(e.get("item_desc")),
                        "cases": 0.0,
                        "locations": set(),
                    },
                )
                if not meta.get("item_desc"):
                    meta["item_desc"] = norm(e.get("item_desc"))
                meta["cases"] = float(meta.get("cases", 0.0)) + c
                if loc:
                    meta["locations"].add(loc)

            # Choose the most common shift_label.
            shift_label = "Off Shift"
            if shift_counts:
                shift_label = sorted(shift_counts.items(), key=lambda kv: kv[1], reverse=True)[0][0]

            # If the delivery was notified, prefer the shift label that was used
            # at notification time (prevents drift across shift changes).
            anchored = str(shift_map.get(d, "")).strip()
            if anchored:
                shift_label = anchored

            item_rows = list(items.values())
            for it in item_rows:
                it["locations"] = sorted(list(it.get("locations", set())))
            item_rows.sort(key=lambda x: float(x.get("cases", 0.0)), reverse=True)

            # Minimum cases filter for Deliveries page
            if min_cases_per_delivery > 0 and total_cases < min_cases_per_delivery:
                continue

            loc_list = sorted(list(locs))

            # Latest detected_at (best-effort)
            latest_detected_epoch = 0
            for e in evs:
                dt_s = norm(e.get("detected_at"))
                if not dt_s:
                    continue
                try:
                    # detected_at is local ISO without timezone
                    epoch = int(time.mktime(time.strptime(dt_s, "%Y-%m-%dT%H:%M:%S")))
                    if epoch > latest_detected_epoch:
                        latest_detected_epoch = epoch
                except Exception:
                    continue

            triage = triage_map.get(d, {})
            if not isinstance(triage, dict):
                triage = {}

            rows.append(
                {
                    "delivery_number": d,
                    "shift_label": shift_label,
                    "notified_at_epoch": int(notified_epoch) if str(notified_epoch).isdigit() else None,
                    "notified_at_local": to_local(notified_epoch) if is_notified else "",
                    "latest_detected_at_epoch": latest_detected_epoch,
                    "latest_detected_at_local": to_local(latest_detected_epoch) if latest_detected_epoch else "",
                    "total_cases": round(total_cases, 2),
                    "locations": loc_list,
                    "items": item_rows,

                    # DC/AM triage fields
                    "triage_checked": bool(triage.get("checked", False)),
                    "triage_primary_cause": _normalize_primary_cause(triage.get("primary_cause", "")),
                    "triage_escalation": str(triage.get("escalation", "") or ""),
                    "triage_note": str(triage.get("note", "") or ""),
                    "triage_updated_by": str(triage.get("updated_by", "") or ""),
                    "triage_updated_at_epoch": int(triage.get("updated_at_epoch") or 0) if str(triage.get("updated_at_epoch") or "").isdigit() else 0,
                    "triage_updated_at_local": to_local(triage.get("updated_at_epoch")) if triage.get("updated_at_epoch") else "",

                    # QA placeholders (not implemented yet)
                    "qa_status": str(triage.get("qa_status", "") or ""),
                    "qa_note": str(triage.get("qa_note", "") or ""),
                }
            )

        # Sort newest first
        if mode == "notified":
            rows.sort(key=lambda r: int(r.get("notified_at_epoch") or 0), reverse=True)
        else:
            rows.sort(key=lambda r: int(r.get("latest_detected_at_epoch") or 0), reverse=True)

        return jsonify({"ok": True, "mode": mode, "rows": rows[:limit]})

    @app.get("/api/triage-options")
    def api_triage_options() -> Response:
        try:
            cfg = _load_config(base_dir)
            triage = cfg.get("triage", {}) if isinstance(cfg, dict) else {}
        except Exception:
            triage = {}
        return jsonify(
            {
                "ok": True,
                "primary_causes": triage.get("primary_causes", []),
                "escalation_options": triage.get("escalation_options", []),
                "qa_status_options": triage.get("qa_status_options", []),
            }
        )

    @app.get("/api/triage")
    def api_triage_get() -> Response:
        payload = _load_triage()
        delivery = str(request.args.get("delivery", "")).strip()
        if delivery:
            deliveries = payload.get("deliveries", {}) if isinstance(payload, dict) else {}
            if not isinstance(deliveries, dict):
                deliveries = {}
            return jsonify({"ok": True, "delivery_number": delivery, "triage": deliveries.get(delivery, {})})
        return jsonify({"ok": True, "triage": payload})

    @app.post("/api/triage")
    def api_triage_post() -> Response:
        try:
            incoming = request.get_json(force=True)
        except Exception:
            return jsonify({"ok": False, "error": "Invalid JSON"}), 400

        if not isinstance(incoming, dict):
            return jsonify({"ok": False, "error": "Invalid payload"}), 400

        delivery_number = str(incoming.get("delivery_number", "")).strip()
        if not delivery_number:
            return jsonify({"ok": False, "error": "delivery_number is required"}), 400

        updates = dict(incoming)
        updates.pop("delivery_number", None)

        # Normalize
        if "checked" in updates:
            updates["checked"] = bool(updates.get("checked"))
        if "note" in updates:
            updates["note"] = str(updates.get("note") or "")[:300]

        try:
            saved = _upsert_delivery_triage(delivery_number=delivery_number, updates=updates)
            return jsonify({"ok": True, "delivery_number": delivery_number, "triage": saved})
        except Exception as e:
            return jsonify({"ok": False, "error": str(e)}), 400

    @app.get("/api/events")
    def api_events() -> Response:
        if not events_log_path.exists():
            return jsonify({"events": []})

        try:
            payload: dict[str, Any] = json.loads(events_log_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            return jsonify({"events": []})

        # Always filter overflow/exempt locations at the API layer too.
        # This prevents old cached events from showing up after config changes.
        try:
            cfg = _load_config(base_dir)
            overflow = {str(x).strip().upper() for x in (cfg.get("monitoring", {}).get("overflow_locations", []) or [])}
        except Exception:
            overflow = set()

        events = payload.get("events", []) if isinstance(payload, dict) else []
        if overflow and isinstance(events, list):
            filtered = []
            for e in events:
                if not isinstance(e, dict):
                    continue
                loc = str(e.get("location_id", "")).strip()
                if loc and loc.strip().upper() in overflow:
                    continue
                filtered.append(e)
            payload["events"] = filtered

        return jsonify(payload)

    @app.get("/api/status")
    def api_status() -> Response:
        if not status_path.exists():
            return jsonify(
                {
                    "state": "starting",
                    "facility_id": None,
                    "tableau_url": "",
                    "last_query_start": None,
                    "last_query_end": None,
                    "last_error": None,
                }
            )
        try:
            payload: dict[str, Any] = json.loads(status_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            payload = {"state": "starting"}
        return jsonify(payload)

    def _cached_analytics_response(
        *,
        cache_path: Path,
        want_refresh: bool,
        min_refresh_seconds: int,
        build_rows: callable,
        now_epoch: int,
    ) -> Response:
        cached: dict[str, Any] | None = None
        if cache_path.exists():
            try:
                cached = json.loads(cache_path.read_text(encoding="utf-8"))
            except json.JSONDecodeError:
                cached = None

        last_refresh = int((cached or {}).get("refreshed_at_epoch", 0))
        can_refresh = (now_epoch - last_refresh) >= min_refresh_seconds

        if want_refresh and not can_refresh:
            wait_s = min_refresh_seconds - (now_epoch - last_refresh)
            return jsonify(
                {
                    "ok": True,
                    "cached": True,
                    "refreshed_at_epoch": last_refresh,
                    "min_refresh_seconds": min_refresh_seconds,
                    "message": f"Refresh throttled. Try again in ~{max(1, wait_s)}s.",
                    "rows": (cached or {}).get("rows", []),
                }
            )

        if want_refresh or cached is None:
            try:
                rows = build_rows()
                payload = {
                    "ok": True,
                    "cached": False,
                    "refreshed_at_epoch": now_epoch,
                    "rows": rows,
                }
                cache_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
                return jsonify(payload)
            except Exception as e:
                if cached is not None:
                    cached["ok"] = True
                    cached["cached"] = True
                    cached["message"] = f"Refresh failed; using cache: {e}"
                    return jsonify(cached)
                return jsonify({"ok": False, "error": str(e), "rows": []}), 500

        return jsonify(cached)

    @app.get("/api/top-items")
    def api_top_items() -> Response:
        cfg = _load_config(base_dir)
        top_cfg = cfg.get("analytics", {}).get("top_items", {})

        days = int(top_cfg.get("days", 30))
        limit = int(top_cfg.get("limit", 25))
        min_refresh_seconds = int(top_cfg.get("min_refresh_seconds", 300))
        cache_path = base_dir / str(top_cfg.get("cache_file", "top_items_cache.json"))

        want_refresh = request.args.get("refresh", "0") == "1"
        now_epoch = int(time.time())

        def build_rows() -> list[dict[str, Any]]:
            facility_id = str(cfg.get("monitoring", {}).get("facility_id", "US-07377"))

            sql = f"""
WITH
  r_latest AS (
    SELECT
      r.CONTAINER_ID,
      CAST(r.ITEM_NBR AS STRING) AS item_nbr,
      SAFE_DIVIDE(r.ITEM_QTY, NULLIF(r.VNPK_QTY, 0)) AS case_qty,
      r.ENTITY_OPERATION_TS
    FROM `wmt-edw-prod.US_SUPPLY_CHAIN_SCT_NONCAT_VM.RECEIVING_ITEM` r
    WHERE r.FACILITY = '{facility_id}'
      AND r.RCV_SET_ON_CONVEYOR_IND = TRUE
      AND r.CONTAINER_ID <> r.MESSAGE_ID
      AND r.ENTITY_OPERATION_TS >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {days} DAY)
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY r.CONTAINER_ID, r.ITEM_NBR
      ORDER BY r.ENTITY_OPERATION_TS DESC
    ) = 1
  ),
  c_filtered AS (
    SELECT
      c.CONTAINER_ID,
      CAST(c.ITEM_NBR AS STRING) AS item_nbr,
      ANY_VALUE(CAST(c.ITEM_DESC AS STRING)) AS item_desc,
      ANY_VALUE(c.DELIVERY_NUMBER) AS DELIVERY_NUMBER
    FROM `wmt-edw-prod.US_SUPPLY_CHAIN_SCT_NONCAT_VM.CONTAINER_ITEM_OPERATIONS` c
    WHERE c.FACILITY = '{facility_id}'
      AND c.CONTAINER_CREATE_TS >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {days} DAY)
      AND c.CONTAINER_ID IN (SELECT CONTAINER_ID FROM r_latest)
    GROUP BY c.CONTAINER_ID, c.ITEM_NBR
  ),
  vendor_map AS (
    SELECT
      d.DELIVERY_NUMBER,
      ANY_VALUE(o.VNDR_NAME) AS vendor_name
    FROM `wmt-edw-prod.US_SUPPLY_CHAIN_SCT_NONCAT_VM.DELIVERY_DOC` d
    LEFT JOIN `wmt-cp-prod.TRANS.ICC_ORD_SCH` o
      ON d.OMS_PO_NBR = CAST(o.OMS_PO_NBR AS STRING)
    WHERE d.DELIVERY_NUMBER IN (SELECT DELIVERY_NUMBER FROM c_filtered)
    GROUP BY d.DELIVERY_NUMBER
  )

SELECT
  COALESCE(v.vendor_name, '') AS vendor_name,
  r.item_nbr AS item_nbr,
  ANY_VALUE(c.item_desc) AS item_desc,
  SUM(r.case_qty) AS total_cases
FROM r_latest r
JOIN c_filtered c
  ON r.CONTAINER_ID = c.CONTAINER_ID
 AND r.item_nbr = c.item_nbr
LEFT JOIN vendor_map v
  ON c.DELIVERY_NUMBER = v.DELIVERY_NUMBER
GROUP BY vendor_name, item_nbr
ORDER BY total_cases DESC
LIMIT {limit}
""".strip()

            return _run_bq_json(cfg, sql, timeout_s=900)

        resp = _cached_analytics_response(
            cache_path=cache_path,
            want_refresh=want_refresh,
            min_refresh_seconds=min_refresh_seconds,
            build_rows=build_rows,
            now_epoch=now_epoch,
        )
        payload = resp.get_json() if hasattr(resp, "get_json") else None
        # enrich with metadata
        if isinstance(payload, dict):
            payload.setdefault("days", days)
            payload.setdefault("limit", limit)
            payload["days"] = days
            payload["limit"] = limit
            return jsonify(payload)
        return resp

    return app


def main() -> None:
    base_dir = Path(__file__).resolve().parent
    port = _get_env_int("ATC_PORT", 5000)
    host = os.environ.get("ATC_HOST", "127.0.0.1")

    app = create_app(base_dir)
    # No debug reloader: it spawns an extra process and breaks “silent” mode.
    app.run(host=host, port=port, debug=False, use_reloader=False)


if __name__ == "__main__":
    main()
